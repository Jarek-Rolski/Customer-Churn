{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac26dec7-0e24-4d8b-835b-306a866ed4b1",
   "metadata": {},
   "source": [
    "# \"The Thera bank\" Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d00a3-af0d-4040-98e0-0c79823b791b",
   "metadata": {},
   "source": [
    "In these notebook I create XGBoost model predicting customers willing to churn. Metric of my concern is Logloss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae2711-fd73-4ac6-aa07-3e1504be587d",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ab54233-4957-415b-9f38-359ac138dadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperopt in /opt/conda/lib/python3.9/site-packages (0.2.7)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from hyperopt) (4.62.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from hyperopt) (1.21.5)\n",
      "Requirement already satisfied: py4j in /opt/conda/lib/python3.9/site-packages (from hyperopt) (0.10.9.3)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.9/site-packages (from hyperopt) (2.6.3)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.9/site-packages (from hyperopt) (0.18.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.9/site-packages (from hyperopt) (2.0.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from hyperopt) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['PWD'],'scripts'))\n",
    "from Data_Prep import Data_Prep\n",
    "from utils import get_metrics_score\n",
    "from utils import make_confusion_matrix\n",
    "\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "import ray\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71fa48-baa4-434d-98f4-3acdabd28e92",
   "metadata": {},
   "source": [
    "## Load and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb43cbcd-9dae-4c4c-ae18-50539d005ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(os.environ['PWD'],'data/train.csv'))\n",
    "\n",
    "X_train = train.drop('Attrition_Flag',axis=1)\n",
    "y_train = train['Attrition_Flag'].map({'Existing Customer': 0, 'Attrited Customer': 1}).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e5ba1-1563-418a-ba23-da873fe36529",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning of XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d983f39a-88d8-4625-91db-482c70152c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 12:32:41,449\tWARNING services.py:1856 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 66957312 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.62gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    }
   ],
   "source": [
    "# Prepare data reference for tuning client\n",
    "object_ref_X_train = ray.put(X_train)\n",
    "object_ref_y_train = ray.put(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f22a40-63db-444f-8532-5df25728756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_scorer = metrics.make_scorer(metrics.log_loss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e55cef-6317-4c42-93d2-fdc74ea51fbf",
   "metadata": {},
   "source": [
    "Hyperparameter tuning using ray-tune library and optimization algorithm HyperOptSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36ee6b8-07ae-4899-9476-2adc333d76fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-11 12:32:44 (running for 00:00:00.13)<br>Memory usage on this node: 2.9/12.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/6.59 GiB heap, 0.0/3.29 GiB objects<br>Current best trial: 104f8ff4 with Logloss Test=-0.08381664571111329 and parameters={'n_estimators': 110, 'gamma': 1.5379798088848569, 'subsample': 0.79, 'colsample_bytree': 0.79, 'colsample_bylevel': 0.9400000000000001, 'learning_rate': 0.1}<br>Result logdir: /usr/src/app/data/ray_results/XGBoost_ros_tuning_2<br>Number of trials: 48/48 (48 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 12:32:44,229\tINFO tune.py:636 -- Total run time: 1.02 seconds (0.00 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "def training_function(config):\n",
    "    # Hyperparameters\n",
    "    config['n_estimators'] = int(config['n_estimators'])      \n",
    "    config['learning_rate'] = 10 ** config['learning_rate']\n",
    "                                           \n",
    "    X_train=ray.get(object_ref_X_train)\n",
    "    y_train=ray.get(object_ref_y_train)\n",
    "    \n",
    "    sys.path.append( os.path.join(os.environ['PWD'],'scripts'))\n",
    "    from Data_Prep import Data_Prep\n",
    "\n",
    "    estimator = Pipeline([('dp', Data_Prep()), ('ros', RandomOverSampler()), \n",
    "                          ('xgb', XGBClassifier(random_state=1, eval_metric='logloss', use_label_encoder=False, **config))])\n",
    "    \n",
    "    cv_results = cross_validate(estimator, X_train, y_train, scoring=ll_scorer, cv=3, return_train_score=True)\n",
    "\n",
    "    d = {'Logloss Training':np.mean(cv_results['train_score']), 'Logloss Test':np.mean(cv_results['test_score'])}\n",
    "    \n",
    "    tune.report(**d)\n",
    "\n",
    "config = {\n",
    "    \"n_estimators\": tune.randint(80, 120),\n",
    "    \"gamma\": tune.uniform(0, 3),\n",
    "    \"subsample\": tune.quniform(0.7, 0.95, 0.01),\n",
    "    \"colsample_bytree\": tune.quniform(0.7, 0.95, 0.01),\n",
    "    \"colsample_bylevel\": tune.quniform(0.7, 0.95, 0.01),    \n",
    "    \"learning_rate\": tune.quniform(-2.0, -1.0, 0.2),  # powers of 10\n",
    "}\n",
    "analysis = tune.run(\n",
    "    training_function,\n",
    "    config=config, \n",
    "    metric='Logloss Test',\n",
    "    mode=\"max\",\n",
    "    num_samples=48,\n",
    "    search_alg=HyperOptSearch(random_state_seed=1),\n",
    "    resume =  'AUTO',#\"ERRORED_ONLY\", \n",
    "    name='XGBoost_ros_tuning', \n",
    "    local_dir=os.path.join(os.environ['PWD'],'data/ray_results'),\n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6c554e7-11bc-4b50-8e7d-4bd65c1781b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('dp', Data_Prep()), ('ros', RandomOverSampler()),\n",
       "                ('xgb',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=0.9400000000000001,\n",
       "                               colsample_bynode=1, colsample_bytree=0.79,\n",
       "                               enable_categorical=False, eval_metric='logloss',\n",
       "                               gamma=1.5379798088848569, gpu_id=-1,\n",
       "                               importance_type=None, interaction_constraints='',\n",
       "                               learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=110,\n",
       "                               n_jobs=12, num_parallel_tree=1, predictor='auto',\n",
       "                               random_state=1, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=1, subsample=0.79,\n",
       "                               tree_method='exact', use_label_encoder=False,\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = analysis.best_config\n",
    "config['learning_rate'] = 10 ** config['learning_rate']\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "estimator = Pipeline([('dp', Data_Prep()), ('ros', RandomOverSampler()), \n",
    "                          ('xgb', XGBClassifier(random_state=1, eval_metric='logloss', use_label_encoder=False, **config))])\n",
    "\n",
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a9a0bdf-2c65-4e33-badd-001522f16101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimator pickled successfully!\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(os.environ['PWD'],'models/xgboost_ros.pkl')\n",
    "pickling_on = open(model_path,\"wb\")\n",
    "pickle.dump(estimator, pickling_on)\n",
    "pickling_on.close()\n",
    "print('estimator pickled successfully!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
